# Core FastAPI
fastapi>=0.104.0,<0.115.0
uvicorn[standard]>=0.24.0,<0.32.0
pydantic>=2.5.0,<3.0.0
python-multipart>=0.0.6
python-dotenv>=1.0.0
httpx>=0.25.0
aiofiles>=23.0.0

# Document Processing

# ML/AI - Core
# NOTE: PyTorch 2.6+ with CUDA 12.4 installed in Dockerfile for RTX 5090 sm_120 support
# Do NOT add PyTorch here - it MUST be installed from cu124 index for native CUDA 12.4 support
# Adding it here risks pip reinstalling from PyPI with wrong binaries (cpu-only or cu121)
# See Dockerfile lines 47-53 for PyTorch installation
# transformers 4.38.0+ fixes frozenset bug with BitsAndBytes on CPU fallback
transformers>=4.38.0,<4.50.0
# accelerate 1.0.0+ requires NumPy 2.x - we're using NumPy 2.x for PyTorch 2.6+
accelerate>=1.0.0,<2.0.0
# BitsAndBytes 0.45.0 has native CUDA 12.4 support (no BNB_CUDA_VERSION override needed!)
# Compatible with PyTorch 2.0-2.5.x and native RTX 5090 Blackwell support
# Improved quantization quality and better memory management vs 0.42.0
bitsandbytes==0.45.0

# ML/AI - Supporting
safetensors>=0.4.3
sentencepiece>=0.1.99
protobuf>=4.25.0,<6.0.0
huggingface-hub>=0.23.0

# scipy 1.13.0+ requires NumPy 2.x (which we use for PyTorch 2.6+)
scipy>=1.13.0,<1.15.0

# Image Processing - NumPy 2.x compatible versions
# opencv-python and scikit-image not needed for LLM-only endpoint
# Removed to reduce dependencies and avoid potential NumPy conflicts

# Utilities
requests>=2.31.0
# NOTE: Will be installed AFTER scipy in Dockerfile to prevent ufunc errors
numpy>=2.0.0,<3.0.0  # NumPy 2.x required for PyTorch 2.6+

# Build dependencies (needed for compilation)
packaging>=23.0
wheel>=0.40.0
setuptools>=65.0

# Flash Attention 2 installed separately in Dockerfile (optional optimization)
# flash-attn>=2.5.0

# RunPod
runpod>=1.3.0
