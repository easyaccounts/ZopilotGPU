# Core FastAPI
fastapi>=0.104.0,<0.115.0
uvicorn[standard]>=0.24.0,<0.32.0
pydantic>=2.5.0,<3.0.0
python-multipart>=0.0.6
python-dotenv>=1.0.0
httpx>=0.25.0
aiohttp>=3.9.0  # For async HTTP callbacks to backend
aiofiles>=23.0.0

# Document Processing

# ML/AI - Core
# NOTE: PyTorch 2.8.0 with CUDA 12.9 installed in Dockerfile for RTX 5090 sm_120 support
# Do NOT add PyTorch here - it MUST be installed from cu129 index for sm_120 support
# Adding it here risks pip reinstalling from PyPI with wrong binaries (cpu-only or wrong CUDA)
# See Dockerfile for PyTorch 2.8.0 installation
# transformers 4.38.0+ fixes frozenset bug with BitsAndBytes on CPU fallback
transformers>=4.38.0,<4.50.0
# accelerate 1.0.0+ requires NumPy 2.x - we're using NumPy 2.x for PyTorch 2.8+
accelerate>=1.0.0,<2.0.0
# BitsAndBytes 0.48.0 for PyTorch 2.8.0 compatibility (released 2 weeks ago)
# Compatible with PyTorch 2.3-2.8+, CUDA 13.0 support, no BNB_CUDA_VERSION override needed
# Intel GPU/Gaudi support, improved 4bit dequantization performance
bitsandbytes==0.48.0

# ML/AI - Supporting
safetensors>=0.4.3
sentencepiece>=0.1.99
protobuf>=4.25.0,<6.0.0
huggingface-hub>=0.23.0

# scipy 1.13.0+ requires NumPy 2.x (which we use for PyTorch 2.6+)
scipy>=1.13.0,<1.15.0

# Image Processing - NumPy 2.x compatible versions
# opencv-python and scikit-image not needed for LLM-only endpoint
# Removed to reduce dependencies and avoid potential NumPy conflicts

# Utilities
requests>=2.31.0
# NOTE: Will be installed AFTER scipy in Dockerfile to prevent ufunc errors
numpy>=2.0.0,<3.0.0  # NumPy 2.x required for PyTorch 2.6+

# Build dependencies (needed for compilation)
packaging>=23.0
wheel>=0.40.0
setuptools>=65.0

# Flash Attention 2 installed separately in Dockerfile (optional optimization)
# flash-attn>=2.5.0

# RunPod
runpod>=1.3.0

# Outlines - Grammar-constrained generation for guaranteed valid JSON
outlines>=0.0.44
